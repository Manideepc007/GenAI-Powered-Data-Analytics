Step 1:

## âœ… 1. **Identify Missing or Inconsistent Data**

### ğŸ” **Missing Values**

| Column         | Missing | % Missing | Notes                                                                           |
| -------------- | ------- | --------- | ------------------------------------------------------------------------------- |
| `Income`       | 39      | 7.8%      | May skew income-based ratios like `Debt_to_Income_Ratio`. Imputation is needed. |
| `Credit_Score` | 2       | 0.4%      | Minor â€” can drop or impute safely.                                              |
| `Loan_Balance` | 29      | 5.8%      | Could impact financial metrics and risk calculations.                           |

> ğŸ› ï¸ **Action**: Consider imputation (e.g., median) or use models that can handle missing values.

---

### âš ï¸ **Inconsistent or Risky Values**

#### ğŸ”¹ **Credit Utilization**

* **Max = 1.03 (or 103%)**, which is **over the safe limit** (normally should be < 1.00).
* This may indicate **overspending beyond limits**, a common delinquency signal.

#### ğŸ”¹ **Debt\_to\_Income\_Ratio**

* **Max = 0.55 (55%)**, which is quite high.
* Industry considers >40% as risky for repayment.

#### ğŸ”¹ **Missed Payments**

* **Range = 0 to 6** â†’ High values like 5 or 6 are strong red flags.
* Consistently high values could distort average behavior metrics.

#### ğŸ”¹ **Monthly History (Month\_1 to Month\_6)**

* Values like **"Late"**, **"Missed"**, **"On-time"**.
* These are text strings and need to be **converted into numerical scores** for modeling.
* Potential inconsistencies if a customer is always â€œLateâ€ but marked non-delinquent.

> ğŸ› ï¸ **Action**: Create a derived metric like:

```python
monthly_cols = ['Month_1', 'Month_2', 'Month_3', 'Month_4', 'Month_5', 'Month_6']
late_score_map = {'On-time': 0, 'Late': 1, 'Missed': 2}
df['Payment_Risk_Score'] = df[monthly_cols].replace(late_score_map).sum(axis=1)
```

---

## ğŸš¨ 2. **Early Risk Indicators: Variables Most Relevant for Predicting Delinquency**

Here are the **top 3 variables** most likely to predict delinquency based on domain logic and data profile:

### ğŸ¥‡ **Missed\_Payments**

* Direct and intuitive signal of past repayment behavior.
* Mean = \~3, Max = 6 â†’ enough variation for modeling.
* Highly predictive of risk.

---

### ğŸ¥ˆ **Credit\_Utilization**

* Max = 103% shows some users are spending beyond their limits.
* Correlates with financial stress and likelihood of default.
* Industry recommendation: keep <30%; values above 50% are concerning.

---

### ğŸ¥‰ **Debt\_to\_Income\_Ratio**

* Indicates how much of the customerâ€™s income is already tied up in debt.
* High DTI = Less capacity to handle more debt.
* Mean = \~30%; anything over 40â€“50% is flagged as risky.

---

### ğŸ¯ Bonus Indicators (supporting role):

* **Payment\_Risk\_Score (derived)** from `Month_1` to `Month_6`.
* **Credit\_Score**: Already a summary of credit behavior, though can be circular if included in predictive scoring.
* **Loan\_Balance vs Income**: High balances with low income signal financial strain.

---

### âœ… Recommendations:

* Impute missing `Income`, `Loan_Balance`, and `Credit_Score`.
* Engineer new features from monthly payment history.
* Normalize outliers in `Credit_Utilization` and `Debt_to_Income_Ratio`.

Let me know if you'd like help creating a **target label** (e.g., using `Delinquent_Account`) or building a simple risk model next!


step 2:

### âœ… **1. Imputation Strategy for This Dataset (Industry Best Practices)**

| Column            | Imputation Method                              | Justification                                                                            |
| ----------------- | ---------------------------------------------- | ---------------------------------------------------------------------------------------- |
| **Income**        | Synthetic generation using normal distribution | Ensures realistic, statistically valid values while preserving overall income pattern.   |
| **Loan\_Balance** | Median imputation                              | Skewed distribution likely; median is robust to outliers and preserves central tendency. |
| **Credit\_Score** | Mean or mode imputation                        | Low missingness (\~0.4%) allows for simple imputation without distorting patterns.       |

---

### âœ… **2. Handling Missing `Credit_Utilization` Values**

**Best-Practice Methods:**

* **Median Imputation** within subgroups (e.g., by `Credit_Card_Type` or `Account_Tenure`)
  â†’ Helps maintain fairness by respecting variability across customer types.
* **Predictive Imputation** using regression or KNN based on related features (`Loan_Balance`, `Debt_to_Income_Ratio`)
  â†’ Maintains logical consistency with correlated financial indicators.


---

### âœ… **3. Generate Realistic Synthetic Income Values**

Assume `Income` follows a **normal distribution** based on existing (non-missing) values:

```python
import numpy as np

# Get mean and std from existing income data
mean_income = df['Income'].mean()
std_income = df['Income'].std()

# Number of missing entries
missing_count = df['Income'].isnull().sum()

# Generate synthetic values
synthetic_income = np.random.normal(loc=mean_income, scale=std_income, size=missing_count)

# Fill the missing income values
df.loc[df['Income'].isnull(), 'Income'] = synthetic_income
```

> This approach maintains the **distribution pattern**, adds variability, and avoids introducing bias.

---

### ğŸ“‹ **Action Table: Handling Key Missing Data**

| Column            | Handling Method                              | Justification                                                           |
| ----------------- | -------------------------------------------- | ----------------------------------------------------------------------- |
| **Income**        | Synthetic generation via normal distribution | Preserves statistical realism while avoiding bias from constant values. |
| **Loan\_Balance** | Median imputation                            | Robust to skew/outliers; better than mean for financial data.           |
| **Credit\_Score** | Mean imputation                              | Very few missing values; quick and safe with minimal impact.            |

---

Let me know if you'd like to apply these steps in Python code across your actual dataset!


overall:

Here is the complete Python code used throughout the EDA process based on your Word report:

---

### âœ… **1. Importing Libraries and Reading Data**

```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_excel("Delinquency_prediction_dataset.xlsx")
```

---

### âœ… **2. Handling Missing Values**

```python
# 1. Impute Income using synthetic normal distribution
df['Income'] = df['Income'].fillna(
    np.random.normal(
        df['Income'].mean(),
        df['Income'].std(),
        df['Income'].isnull().sum()
    )
)

# 2. Impute Loan_Balance using median
df['Loan_Balance'] = df['Loan_Balance'].fillna(df['Loan_Balance'].median())

# 3. Impute Credit_Score using mean
df['Credit_Score'] = df['Credit_Score'].fillna(df['Credit_Score'].mean())
```

---

### âœ… **3. Feature Engineering: Payment Risk Score**

```python
# Define mapping and monthly columns
monthly_cols = ['Month_1', 'Month_2', 'Month_3', 'Month_4', 'Month_5', 'Month_6']
late_score_map = {'On-time': 0, 'Late': 1, 'Missed': 2}

# Create Payment_Risk_Score feature
df['Payment_Risk_Score'] = df[monthly_cols].replace(late_score_map).sum(axis=1)
```

---

### âœ… **4. (Optional) Correlation Matrix & Risk Indicator Exploration**

```python
# Check correlation of numerical features with Delinquent_Account
correlations = df.corr(numeric_only=True)['Delinquent_Account'].sort_values(ascending=False)
print(correlations)
```

---

Let me know if you'd like a `.py` file download or help running a classification model next!


Hereâ€™s the Python code to **identify missing or inconsistent data** in your dataset. This includes:

1. Columns with missing values
2. Rows with multiple missing entries
3. Potential outliers or invalid values (e.g., credit utilization > 1.0)

---

### âœ… **Step-by-Step Code to Identify Missing or Inconsistent Data**

```python
# 1. Check total missing values per column
missing_summary = df.isnull().sum()
missing_summary = missing_summary[missing_summary > 0].sort_values(ascending=False)
print("ğŸ” Missing values per column:\n", missing_summary)

# 2. Count rows with more than 1 missing field (optional threshold)
rows_with_multiple_missing = df[df.isnull().sum(axis=1) > 1]
print(f"\nğŸ“Œ Number of rows with >1 missing values: {len(rows_with_multiple_missing)}")

# 3. Check for inconsistent or unexpected values

# Example: Credit Utilization should be between 0 and 1 (or 0% and 100%)
invalid_utilization = df[df['Credit_Utilization'] > 1.0]
print(f"\nâš ï¸ Records with Credit_Utilization > 1.0: {len(invalid_utilization)}")

# Optional: Preview the first few inconsistent entries
print(invalid_utilization[['Customer_ID', 'Credit_Utilization']].head())

# 4. Check for negative or zero income values (if not logically possible)
invalid_income = df[df['Income'] <= 0]
print(f"\nâš ï¸ Records with non-positive Income: {len(invalid_income)}")
```

---

### ğŸ” **What This Helps You Catch**

* Missing values that may need imputation or removal
* Multiple gaps in a single row that may compromise data quality
* Invalid financial values (e.g., utilization > 100%, income â‰¤ 0)

Let me know if you'd like this combined with previous steps or exported into a `.py` script.


